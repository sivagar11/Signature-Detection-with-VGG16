# -*- coding: utf-8 -*-
"""signature_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gxYbUMUfgC4YCdsyDPHxdo2qOt57_9Be
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Dense, Flatten , Dropout
from tensorflow.keras.models import Model

# Load the VGG16 model with pre-trained weights (excluding the top fully connected layers)
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Add custom classification layers
x = base_model.output
x = Flatten()(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
predictions = Dense(2, activation='softmax')(x)

# Create the model
model = Model(inputs=base_model.input, outputs=predictions)

for layer in base_model.layers:
    layer.trainable = False

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Set up data generators with data augmentation
train_datagen = ImageDataGenerator(
    preprocessing_function=preprocess_input,
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True,
    vertical_flip=True,
    fill_mode='constant'
)

train_generator = train_datagen.flow_from_directory(
    '/content/drive/MyDrive/Farrer-hos/box classification',  # Replace with the path to your training data
    target_size=(224, 224),
    class_mode='categorical',  # Assuming you have categorical labels
)

# Train the model
model.fit(train_generator, epochs=5)# Save the trained model

model.save('signature_model_box.h5')

# Get the mapping of numeric labels to class names
class_names = list(train_generator.class_indices.keys())

print(class_names)

import os
import tensorflow as tf
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.applications.vgg16 import preprocess_input
import numpy as np

def predict_class(image_path, model):
    # Load and preprocess an image for testing
    def preprocess_image(image_path):
        img = load_img(image_path, target_size=(224, 224))
        img_array = img_to_array(img)
        img_array = np.expand_dims(img_array, axis=0)
        img_array = preprocess_input(img_array)
        return img_array

    preprocessed_image = preprocess_image(image_path)

    prediction = model.predict(preprocessed_image)

    predicted_class = np.argmax(prediction)

    return predicted_class

imae = "/content/Screenshot 2023-09-14 at 4.35.08 PM.png"

clss = predict_class(imae,model)

print(clss)





import cv2

def split_and_return_coordinates(image_path):
    # Load the image
    image = cv2.imread(image_path)

    image_height, image_width, _ = image.shape

    # Define the coordinates of the top-left and bottom-right corners of the bounding box
    x1 = int(image_width * 0.64)
    x2 = int(image_width * 0.9)
    y1 = int(image_height * 0.75)
    y2 = int(image_height * 0.88)

    # Calculate the width and height of each small box
    box_width = (x2 - x1) // 3
    box_height = (y2 - y1) // 2

    # Initialize a list to store the coordinates of the small boxes
    small_boxes_list = []

    # Draw 3x2 small boxes on the image and store their coordinates
    for i in range(3):  # 3 rows
        for j in range(2):  # 2 columns
            x1_box = x1 + i * box_width
            y1_box = y1 + j * box_height
            x2_box = x1_box + box_width
            y2_box = y1_box + box_height
            cv2.rectangle(image, (x1_box, y1_box), (x2_box, y2_box), (0, 0, 255), 2)  # Red color
            small_boxes_list.append((x1_box, x2_box, y1_box, y2_box))

    # Save the image with the small boxes drawn on it
    cv2.imwrite('ot.png', image)

    # Return the coordinates of the small boxes
    return small_boxes_list

# Specify the path to the input image
image_path = '/content/FPH Test1_page-0002.jpg'

# Call the function and get the coordinates
coordinates = split_and_return_coordinates(image_path)

# Print the coordinates
for i, (x1_box, x2_box, y1_box, y2_box) in enumerate(coordinates):
    print(f"Box {i+1}: x1={x1_box}, x2={x2_box}, y1={y1_box}, y2={y2_box}")

import cv2
import numpy as np
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.vgg16 import preprocess_input

# Load your pre-trained signature detection model
def load_signature_model(model_path):
    model = load_model(model_path)
    return model

# Function to detect signatures in small boxes using the pre-trained model
def detect_signatures_in_boxes(small_boxes_list, image_path, model):
    # Load the image
    image = cv2.imread(image_path)

    detected_signatures = []

    for i, small_box in enumerate(small_boxes_list):
        xmin, xmax, ymin, ymax = small_box
        small_image = image[ymin:ymax, xmin:xmax]

        # Resize the small image to match the model's input size (e.g., 224x224)
        small_image = cv2.resize(small_image, (224, 224))

        # Preprocess the small image for model input
        #small_image = preprocess_input(small_image)


        # Use the pre-trained model to predict the class for the small box
        predicted_class = model.predict(np.expand_dims(small_image, axis=0))

        # Get the class label
        class_label = 'signature found' if predicted_class[0, 0] == 0 else 'signature not found'

        # Store the detected signature with class label and coordinates
        detected_signatures.append({
            "box_index": i,
            "class_label": class_label,
            "coordinates": (xmin, xmax, ymin, ymax),
        })

    return detected_signatures

# Example usage:
#image_path = '/Users/sivagar/Documents/projects/farrer_hos/projects/signature/Screenshot 2023-09-14 at 5.01.56 PM.png'
model_path = '/content/signature_model_box.h5'
model = load_signature_model(model_path)


detected_signatures = detect_signatures_in_boxes(coordinates, image_path, model)

# Print the detected signatures
for signature in detected_signatures:
    print(f"Box Index: {signature['box_index']}, Class Label: {signature['class_label']}, Coordinates: {signature['coordinates']}")